{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from dataset import DatasetBuilder\n",
    "from utils import preprocess_sequences_to_fixed_len, standardize_and_turn_tensor\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X.to(device)\n",
    "        self.Y = Y.to(device).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'sequence': self.X[idx, :, :], 'label': self.Y[idx]}\n",
    "        return sample\n",
    "\n",
    "\n",
    "def seq_data_to_dataset(seq_data, cap_len, num_features, device, standardize=True):\n",
    "    X, idx_removed = preprocess_sequences_to_fixed_len(seq_data, cap_len, num_features)\n",
    "    X = standardize_and_turn_tensor(X, standardize=standardize)\n",
    "    Y = torch.from_numpy(np.concatenate([x_y[1] for ix, x_y in enumerate(seq_data) if ix not in idx_removed]))\n",
    "    print(f\"generated tensor datasets of size: X{X.size()}, Y{Y.size()}\")\n",
    "    return SeqDataset(X, Y, device)\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, seq_size, h_size, n_classes=2, n_lstm_layers=1, n_linear_layers_hidden=0,\n",
    "                 dropout=0.):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # self.bn = nn.BatchNorm1d(seq_size, affine=False)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=h_size, num_layers=n_lstm_layers,\n",
    "                            batch_first=True, dropout=dropout,\n",
    "                            bidirectional=False)\n",
    "        self.linear = nn.Sequential(\n",
    "            *chain(*[(nn.Linear(h_size, h_size), nn.ReLU()) for _ in range(n_linear_layers_hidden)]),\n",
    "            nn.Linear(h_size, n_classes))\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        input = seq = torch FloatTensor of size (B, Seqlen, Input_size)\n",
    "        output is of size (B, hidden_size * num layers) -> concatenation of the last hidden state over the LSTM layers\n",
    "        returns = the logits of this output passed to a linear layer\n",
    "        \"\"\"\n",
    "        # seq = self.bn(seq)\n",
    "        out, (_, _) = self.lstm(seq)\n",
    "        return self.linear(out.mean(1))\n",
    "\n",
    "\n",
    "def dataset_iterator(dataset, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    :param dataset: sequential dataset as returned by create_dataset\n",
    "    :param batch_size\n",
    "    :param cap_len: if not None, the cap on the sequence lengths you want to put\n",
    "    :param shuffle: if you want to shuffle the dataset at the beginning of each epoch\n",
    "    :return: yield batches for the training, each of the form [(sequence:TorchTensor, sequence_label:TorchTensor)] with len <=batch_size\n",
    "    each sequence Tensor is of size (<=cap_len or length of the tweets sequence, num_features)\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        random.shuffle(dataset)\n",
    "    count_sampled = 0\n",
    "    n = len(dataset)\n",
    "    while (count_sampled < n):\n",
    "        yield dataset[count_sampled:count_sampled + batch_size]\n",
    "        count_sampled += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, optim, train_loader, test_loader=None):\n",
    "    epoch_ckp = 0\n",
    "    global_step = 0\n",
    "    accuracies = np.zeros(5, dtype=float)\n",
    "    max_running_mean = 0.\n",
    "    # Training phase\n",
    "    loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "    for epoch in range(epoch_ckp, epoch_ckp + args.num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0\n",
    "        for ix, batch in enumerate(train_loader):  # enumerate(dataset_iterator(train_loader, batch_size=args.batch_size, shuffle=True)):\n",
    "            sequences, ys = batch['sequence'], batch['label']\n",
    "            logits = model(sequences.float())  # .float())\n",
    "            \n",
    "#             ipdb.set_trace()\n",
    "            loss = loss_function(logits, ys)\n",
    "            # Optimization\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # Printing running loss\n",
    "            epoch_loss += loss.item()\n",
    "            if not ix:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * 0.5 + loss.item() * 0.5\n",
    "            if args.verbose:\n",
    "                print(f\"Step {ix + 1}, running loss: {running_loss:.4f}\")\n",
    "            # Evaluation on the Validation set every 10 steps\n",
    "            if global_step % 5 == 0:\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                n_samples = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        sequences, ys = batch['sequence'], batch['label']\n",
    "                        _, pred = model(sequences.float()).max(dim=1)\n",
    "                        correct += float(pred.eq(ys).sum().item())\n",
    "                        n_samples += pred.size(0)\n",
    "                acc = correct / n_samples\n",
    "                accuracies = np.concatenate([accuracies[1:], np.array([acc])])\n",
    "                # print(accuracies)\n",
    "                \n",
    "                if accuracies.mean() > max_running_mean:\n",
    "                    max_running_mean = accuracies.mean()\n",
    "                model.train()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"running mean accuracy is {accuracies.mean():.3f}\") \n",
    "            print(\"epoch\", epoch, \"loss:\", epoch_loss / len(train_loader))\n",
    "\n",
    "    return max_running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = {'dataset':\"twitter15\",\n",
    "          'lr':0.0005,\n",
    "          'num_epochs':250,\n",
    "          'num_lstm_layers':2,\n",
    "          'num_linear_layers':1,\n",
    "          'hidden_size':16,\n",
    "          'dropout':0.5,\n",
    "          'batch_size':64,\n",
    "          'debug':0,\n",
    "          'test_on_train':0,\n",
    "          'verbose':0,\n",
    "          'cap_len':40,\n",
    "          'use_cuda':1,\n",
    "         'n_classes':4}\n",
    "args = Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 4 classes problem\n",
      "We consider tweets emitted no later than 1500mins after the root tweet\n",
      "Len train/val/test 1005 149 336\n",
      "Oversampling...\n",
      "Before oversampling: 1490 trees, 1005 train trees\n",
      "After oversampling: 1490 trees, 1005 train trees\n",
      "Dataset loaded in 29.419s\n"
     ]
    }
   ],
   "source": [
    "dataset_builder = DatasetBuilder(args.dataset, only_binary=True if args.n_classes==2 else False, time_cutoff=1500)\n",
    "full_dataset = dataset_builder.create_dataset(dataset_type=\"sequential\", standardize_features=False)\n",
    "val_dataset = full_dataset['val']\n",
    "\n",
    "if args.debug:\n",
    "    train_dataset = val_dataset\n",
    "else:\n",
    "    train_dataset = full_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if args.use_cuda and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length preprocessing: lost 0 sequences that were unit-sized, oversampled 12 sequences\n",
      "Shape of input seq data is ndarry of shape (1005, 40, 11)\n",
      "generated tensor datasets of size: Xtorch.Size([1005, 40, 11]), Ytorch.Size([1005])\n",
      "Fixed-length preprocessing: lost 0 sequences that were unit-sized, oversampled 3 sequences\n",
      "Shape of input seq data is ndarry of shape (149, 40, 11)\n",
      "generated tensor datasets of size: Xtorch.Size([149, 40, 11]), Ytorch.Size([149])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = seq_data_to_dataset(train_dataset, cap_len=args.cap_len, num_features=11, standardize=True, device=device)\n",
    "val_dataset = seq_data_to_dataset(val_dataset, cap_len=args.cap_len, num_features=11, standardize=True, device=device)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size,\n",
    "                        shuffle=True) if not args.test_on_train else train_loader  # to change if different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 246, 1: 246, 2: 255, 3: 258}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico = {key:0 for key in range(4)}\n",
    "for elt in train_dataset.Y:\n",
    "    dico[elt.item()] +=1\n",
    "dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(input_size=11,\n",
    "                       seq_size=args.cap_len,\n",
    "                       h_size=args.hidden_size,\n",
    "                       n_lstm_layers=args.num_lstm_layers,\n",
    "                       n_linear_layers_hidden=args.num_linear_layers - 1,\n",
    "                       n_classes=args.n_classes,\n",
    "                       dropout=args.dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running mean accuracy is 0.412\n",
      "epoch 9 loss: 1.354085698723793\n",
      "running mean accuracy is 0.538\n",
      "epoch 19 loss: 1.1320799365639687\n",
      "running mean accuracy is 0.540\n",
      "epoch 29 loss: 1.0716021843254566\n",
      "running mean accuracy is 0.542\n",
      "epoch 39 loss: 1.0331277251243591\n",
      "running mean accuracy is 0.570\n",
      "epoch 49 loss: 1.0188994593918324\n",
      "running mean accuracy is 0.558\n",
      "epoch 59 loss: 0.9835345484316349\n",
      "running mean accuracy is 0.562\n",
      "epoch 69 loss: 0.9562350474298\n",
      "running mean accuracy is 0.565\n",
      "epoch 79 loss: 0.9322734326124191\n",
      "running mean accuracy is 0.568\n",
      "epoch 89 loss: 0.9112190529704094\n",
      "running mean accuracy is 0.585\n",
      "epoch 99 loss: 0.8793868571519852\n",
      "running mean accuracy is 0.573\n",
      "epoch 109 loss: 0.8617966398596764\n",
      "running mean accuracy is 0.608\n",
      "epoch 119 loss: 0.834518376737833\n",
      "running mean accuracy is 0.626\n",
      "epoch 129 loss: 0.8110368214547634\n",
      "running mean accuracy is 0.615\n",
      "epoch 139 loss: 0.8038013651967049\n",
      "running mean accuracy is 0.632\n",
      "epoch 149 loss: 0.7934549264609814\n",
      "running mean accuracy is 0.632\n",
      "epoch 159 loss: 0.7653359957039356\n",
      "running mean accuracy is 0.628\n",
      "epoch 169 loss: 0.7499703466892242\n",
      "running mean accuracy is 0.644\n",
      "epoch 179 loss: 0.7554228752851486\n",
      "running mean accuracy is 0.623\n",
      "epoch 189 loss: 0.7344604730606079\n",
      "running mean accuracy is 0.644\n",
      "epoch 199 loss: 0.7149316780269146\n",
      "running mean accuracy is 0.627\n",
      "epoch 209 loss: 0.7068810500204563\n",
      "running mean accuracy is 0.647\n",
      "epoch 219 loss: 0.7025074306875467\n",
      "running mean accuracy is 0.624\n",
      "epoch 229 loss: 0.6869401596486568\n",
      "running mean accuracy is 0.639\n",
      "epoch 239 loss: 0.6789462696760893\n",
      "running mean accuracy is 0.635\n",
      "epoch 249 loss: 0.6723662316799164\n"
     ]
    }
   ],
   "source": [
    "perf = train(args, model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644295302013423"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
