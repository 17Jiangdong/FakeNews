{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import DatasetBuilder\n",
    "from utils import preprocess_sequences_to_fixed_len, standardize_and_turn_tensor\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'sequence': self.X[idx, :, :], 'label': self.Y[idx]}\n",
    "        return sample\n",
    "\n",
    "\n",
    "def seq_data_to_dataset(seq_data, cap_len, num_features, standardize=True):\n",
    "    X, idx_removed = preprocess_sequences_to_fixed_len(seq_data, cap_len, num_features)\n",
    "    X = standardize_and_turn_tensor(X, standardize=False)\n",
    "    Y = torch.from_numpy(np.concatenate([x_y[1] for ix, x_y in enumerate(seq_data) if ix not in idx_removed]))\n",
    "    print(f\"generated tensor datasets of size: X{X.size()}, Y{Y.size()}\")\n",
    "    return SeqDataset(X, Y)\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, seq_size, h_size, n_classes=4, n_lstm_layers=1, n_linear_layers_hidden=0,\n",
    "                 dropout=0.):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        # self.bn = nn.BatchNorm1d(seq_size, affine=False)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=h_size, num_layers=n_lstm_layers,\n",
    "                            batch_first=True, dropout=dropout,\n",
    "                            bidirectional=False)\n",
    "        self.linear = nn.Sequential(\n",
    "            *chain(*[(nn.Linear(h_size, h_size), nn.ReLU()) for _ in range(n_linear_layers_hidden)]),\n",
    "            nn.Linear(h_size, n_classes))\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        input = seq = torch FloatTensor of size (B, Seqlen, Input_size)\n",
    "        output is of size (B, hidden_size * num layers) -> concatenation of the last hidden state over the LSTM layers\n",
    "        returns = the logits of this output passed to a linear layer\n",
    "        \"\"\"\n",
    "        # seq = self.bn(seq)\n",
    "        out, (_, _) = self.lstm(seq)\n",
    "        return self.linear(out.mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_len = 40\n",
    "hidden_size = 24\n",
    "num_lstm_layers = 2\n",
    "num_linear_layers = 1\n",
    "dropout_prob = 0.5\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:  # dataset_iterator(test_loader, batch_size=args.batch_size, shuffle=True):\n",
    "            sequences, ys = batch['sequence'], batch['label']\n",
    "            _, pred = model(sequences.float()).max(dim=1)\n",
    "            correct += float(pred.eq(ys).sum().item())\n",
    "            n_samples += pred.size(0)\n",
    "    return correct / n_samples\n",
    "\n",
    "\n",
    "def train(model, optim, train_loader, val_loader=None):\n",
    "    # Training phase\n",
    "    loss_function = nn.CrossEntropyLoss(reduction='mean')\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for ix, batch in enumerate(\n",
    "                train_loader):  # enumerate(dataset_iterator(train_loader, batch_size=args.batch_size, shuffle=True)):\n",
    "            sequences, ys = batch['sequence'], batch['label']\n",
    "            logits = model(sequences.float())  # .float())\n",
    "            loss = loss_function(logits, ys)\n",
    "\n",
    "            # Optimization\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # Printing running loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch{epoch + 1}, Loss: {epoch_loss / len(train_loader):.3f}, Accuracy on Val: {eval_func(model, val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'twitter16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 4 classes problem\n",
      "No time consideration\n",
      "Features that will be considered: text_only\n",
      "Len train/val/test 552 82 184\n"
     ]
    }
   ],
   "source": [
    "dataset_builder = DatasetBuilder(exp_name, only_binary=False, time_cutoff=None, features_to_consider=\"text_only\")\n",
    "full_dataset = dataset_builder.create_dataset(dataset_type=\"sequential\", standardize_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = full_dataset['train']\n",
    "val_dataset = full_dataset['val']\n",
    "test_dataset = full_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in train_dataset:\n",
    "    try:\n",
    "        elt[1].shape\n",
    "    except:\n",
    "        print(elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length preprocessing: lost 0 sequences that were unit-sized, oversampled 0 sequences\n",
      "Shape of input seq data is ndarry of shape (1005, 40, 768)\n",
      "generated tensor datasets of size: Xtorch.Size([1005, 40, 768]), Ytorch.Size([1005])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = seq_data_to_dataset(train_dataset, cap_len=cap_len, num_features=768, standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length preprocessing: lost 0 sequences that were unit-sized, oversampled 0 sequences\n",
      "Shape of input seq data is ndarry of shape (149, 40, 768)\n",
      "generated tensor datasets of size: Xtorch.Size([149, 40, 768]), Ytorch.Size([149])\n",
      "Fixed-length preprocessing: lost 0 sequences that were unit-sized, oversampled 0 sequences\n",
      "Shape of input seq data is ndarry of shape (336, 40, 768)\n",
      "generated tensor datasets of size: Xtorch.Size([336, 40, 768]), Ytorch.Size([336])\n"
     ]
    }
   ],
   "source": [
    "val_dataset = seq_data_to_dataset(val_dataset, cap_len=cap_len, num_features=768, standardize=False)\n",
    "test_dataset = seq_data_to_dataset(test_dataset, cap_len=cap_len, num_features=768, standardize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(input_size=768,\n",
    "                       seq_size=cap_len,\n",
    "                       h_size=hidden_size,\n",
    "                       n_lstm_layers=num_lstm_layers,\n",
    "                       n_linear_layers_hidden=num_linear_layers - 1,\n",
    "                       dropout=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch50, Loss: 0.412, Accuracy on Val: 0.597\n",
      "Epoch100, Loss: 0.245, Accuracy on Val: 0.584\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMtext, twitter15, 100, 64, 0.001, 24, 2, 1, 0.5, 0.8985, 0.5839, 0.6220\n"
     ]
    }
   ],
   "source": [
    "print(f\"LSTMtext, {exp_name}, {num_epochs}, {batch_size}, {lr}, {hidden_size}, {num_lstm_layers}, {num_linear_layers}, {dropout_prob}, {eval_func(model, train_loader):.4f}, {eval_func(model, val_loader):.4f}, {eval_func(model, test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
