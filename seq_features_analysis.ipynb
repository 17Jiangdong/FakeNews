{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We select the cutoff for the \"time since root tweet\" limit in fetching tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"twitter15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering 4 classes problem\n",
      "We consider tweets emitted no later than 10000mins after the root tweet\n",
      "Features that will be considered: user_only\n",
      "Len train/val/test 1005 149 336\n",
      "Oversampling...\n",
      "Before oversampling: 1490 trees, 1005 train trees\n",
      "After oversampling: 1490 trees, 1005 train trees\n",
      "Dataset loaded in 41.678s\n"
     ]
    }
   ],
   "source": [
    "data_builder = ds.DatasetBuilder(exp_name, time_cutoff=CUTOFF)\n",
    "dataset = data_builder.create_dataset(dataset_type=\"raw\", standardize_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = dataset['train']\n",
    "raw_validate = dataset['val']\n",
    "raw_test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feature_names = [\"created_at\",\n",
    "                    \"favourites_count\", \n",
    "                    \"followers_count\", \n",
    "                    \"friends_count\", \n",
    "                    \"geo_enabled\",\n",
    "                    \"has_description\",\n",
    "                    \"len_name\",\n",
    "                    \"len_screen_name\",\n",
    "                    \"listed_count\",\n",
    "                    \"statuses_count\", \n",
    "                    \"verified\"\n",
    "                     ]\n",
    "edge_feature_names = sorted(edge_feature_names)\n",
    "edge_feature_names = [\"label\",\n",
    "                      \"root_id\",\n",
    "                      \"in_tweet_idx\",\n",
    "                      \"out_tweet_idx\",\n",
    "                      \"latency\",\n",
    "                      \"in_uid\",\n",
    "                      \"out_uid\"] + edge_feature_names\n",
    "n_cols = len(edge_feature_names)\n",
    "n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set the prediction latency level for our model: X minutes means our model classifies using features available X minutes after the root tweet is emitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_LATENCY = 120 # in minutes after the first tweet is emitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_raw(raw_data):\n",
    "    data_dict = {name:[] for name in edge_feature_names}\n",
    "    for features_sequence in raw_data:\n",
    "        for dp in features_sequence:\n",
    "            for i in range(n_cols):\n",
    "                data_dict[edge_feature_names[i]].append(dp[i])\n",
    "    df = pd.DataFrame(data=data_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_df_from_raw(raw_train)\n",
    "df_validate = get_df_from_raw(raw_validate)\n",
    "df_test = get_df_from_raw(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = list(df_train.label.unique())\n",
    "lookup_dict = {label:integer for integer, label in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lookup_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train.in_tweet_idx.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df_train.groupby(['out_uid']).nunique().loc[:, ['label', 'favourites_count']]\n",
    "# df_test.favourites_count.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying log transforms where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = ['favourites_count', 'followers_count', 'friends_count', 'statuses_count']\n",
    "for colname in to_log:\n",
    "    df_train.loc[:, colname] = np.log(df_train.loc[:, colname].values + 1)\n",
    "    df_validate.loc[:, colname] = np.log(df_validate.loc[:, colname].values + 1)\n",
    "    df_test.loc[:, colname] = np.log(df_test.loc[:, colname].values + 1)\n",
    "df_train.label = df_train.label.apply(lambda x: lookup_dict[x])\n",
    "df_validate.label = df_validate.label.apply(lambda x: lookup_dict[x])\n",
    "df_test.label = df_test.label.apply(lambda x: lookup_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_by_latency(df):\n",
    "    df = df.loc[df.latency <= PREDICTION_LATENCY]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = cut_by_latency(df_train)\n",
    "# df_validate = cut_by_latency(df_validate)\n",
    "# df_test = cut_by_latency(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train.root_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analysis based on a simple aggregation of the features by root_id\n",
    "- in_tweet -> nunique\n",
    "- latency -> mean\n",
    "- in_uid -> nunique\n",
    "- created_at -> mean\n",
    "- followers_count -> mean\n",
    "- favourites_count -> mean\n",
    "- friends_count -> mean\n",
    "- geo_enabled -> mean\n",
    "- has_description -> mean\n",
    "- statuses_count -> mean\n",
    "- verified -> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_kept = [colname for colname in df_train.columns if colname not in ['len_name', 'len_screen_name', 'listed_count']]\n",
    "cols_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = ['out_tweet_idx', 'out_uid', 'in_tweet_idx', 'in_uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cols = [colname for colname in cols_kept if colname not in count_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_cols = []\n",
    "sum_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cols_kept), len(mean_cols), len(count_cols), len(sum_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[cols_kept]\n",
    "df_validate = df_validate[cols_kept]\n",
    "df_test = df_test[cols_kept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_v0(df):\n",
    "    aggregations_dict = {name:'sum' for name in sum_cols}\n",
    "    aggregations_dict.update({name: 'mean' for name in mean_cols})\n",
    "    aggregations_dict.update({name: 'nunique' for name in count_cols})\n",
    "    aggregated_data = df.groupby('root_id').agg(aggregations_dict)\n",
    "    aggregated_data = aggregated_data.reset_index(drop=True)\n",
    "#     aggregated_data = aggregated_data.drop(columns='root_id')\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We aggregate the features as planned and fit a GB with Decision Trees model, evaluate its ROC-AUC perf on val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = aggregation_v0(df_train)\n",
    "df_validate = aggregation_v0(df_validate)\n",
    "df_test = aggregation_v0(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binned_plot(data, feature_col, target_col, nquantiles=5):\n",
    "    feature_vals = data.loc[:, feature_col].values\n",
    "    target_vals = data.loc[:, target_col].values\n",
    "    quantiles_to_compute = np.linspace(0, 1, num=nquantiles+2)\n",
    "    bin_edges = np.quantile(feature_vals, q=quantiles_to_compute)\n",
    "    nx, _ = np.histogram(feature_vals, bins=bin_edges)\n",
    "    sum_x, _ = np.histogram(feature_vals, bins=bin_edges, weights=feature_vals)\n",
    "    sum_y, _ = np.histogram(feature_vals, bins=bin_edges, weights=target_vals)\n",
    "    plt.plot(sum_x / nx, sum_y/nx, color='black')\n",
    "    plt.xlabel('Feature {}'.format(feature_col))\n",
    "    plt.ylabel('Target mean')\n",
    "    plt.ylim(target_vals.min(), target_vals.max())\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lgb.Dataset(df_train.iloc[:, 1:].drop(columns='root_id'), label=df_train.label)\n",
    "dataset.construct()\n",
    "params = {\n",
    "    'objective':'multiclass',\n",
    "    'num_class':4,\n",
    "    'bagging_freq':5,\n",
    "    'feature_fraction':1.0,\n",
    "    'boosting_type':'gbdt',\n",
    "    'max_depth':5,\n",
    "    'learning_rate':0.005, # range tested is 0.001, 0.01, 0.005\n",
    "    'n_estimators':2000,# range tested is 1000, 2000\n",
    "    'verbosity':2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = lgb.train(params, train_set=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_accuracy(data):\n",
    "    preds = gbm_model.predict(data.iloc[:, 1:].drop(columns='root_id'), raw_score=False).argmax(1)\n",
    "    return accuracy_score(data.iloc[:, 0].values, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LighGBM, {exp_name}, 0.005, 2000, {return_accuracy(df_train):.4f}, {return_accuracy(df_validate):.4f}, {return_accuracy(df_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_builder = ds.DatasetBuilder(exp_name, time_cutoff=CUTOFF, features_to_consider='text_only')\n",
    "dataset = data_builder.create_dataset(dataset_type=\"raw\", standardize_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(raw_dt):\n",
    "    mlp_data = [tree[0] for tree in raw_dt]\n",
    "    mlp_data = np.stack([np.array([dp[1], lookup_dict[dp[0]]] + dp[7:]) for dp in mlp_data])\n",
    "    return mlp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlp = to_dataset(dataset['train'])\n",
    "val_mlp = to_dataset(dataset['val'])\n",
    "test_mlp = to_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_mlp[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layers = 3\n",
    "hidden_size = 24\n",
    "batch_size = 64\n",
    "lr = 0.0005\n",
    "# dropout_prob =0.5\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq = []\n",
    "model_seq.append(nn.Linear(in_features=768, out_features=hidden_size))\n",
    "for _ in range(n_hidden_layers):\n",
    "#     model_seq.append(nn.Dropout(dropout_prob))\n",
    "    model_seq.append(nn.Linear(in_features=hidden_size, out_features=hidden_size))\n",
    "model_seq.append(nn.Linear(in_features=hidden_size, out_features=4))\n",
    "mlp = nn.Sequential(*model_seq)\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(mlp.parameters(), lr=lr) #[param for name, param in mlp.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_mlp[:, 1:], batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_mlp[:, 1:], batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_mlp[:, 1:], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loader(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for dp in loader:\n",
    "            y = dp[:, 0].to(device).long()\n",
    "            x = dp[:, 1:].to(device).float()\n",
    "            logits = mlp(x)\n",
    "            _, preds = logits.max(dim=1)\n",
    "            correct += float(preds.eq(y).sum().item())\n",
    "            total += y.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='mean')\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    #TRAIN\n",
    "    mlp.train()\n",
    "    for dp in train_loader:\n",
    "        y = dp[:, 0].to(device).long()\n",
    "        x = dp[:, 1:].to(device).float()\n",
    "        logits = mlp(x)\n",
    "\n",
    "        \n",
    "        loss = loss_func(logits, y)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Mean Loss = {epoch_loss/len(train_loader):.3f}\")\n",
    "        \n",
    "    \n",
    "    #EVAL\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Accuracy Epoch {epoch+1} on Val: {eval_loader(mlp, val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MLPtext, {exp_name}, 100, 64, 5e-4, 24, 3, {eval_loader(mlp, train_loader):.4f}, {eval_loader(mlp, val_loader):.4f}, {eval_loader(mlp, test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEIZ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
